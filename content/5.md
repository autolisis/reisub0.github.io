Title: Fractal Learning
Date: 2020-10-07
Category: learning
Tags: learning, systems
Status: draft

![tree]({static}/images/tree.gif)

## Part 1 - Complexity

This world is a complex, interconnected web of systems. We've tried to make sense of this by creating various (seemingly-unrelated) disciplines. With the huge number of disciplines, I think you'll agree with me when I say that it's impossible for any one person to be an expert in all of these fields.

It's turtles all the way down, however, and even as you go deeper into a field, there's still so much complexity that you would despair at the idea of ever really understanding anything even within the confines of one specific field.

Most fields seem to exhibit a fractal pattern. By this, I mean that the more you try to *zoom in* onto a specific aspect of a system, the more detail is revealed. The same is true whether you're talking about economics or physics or biology. A whole new world of detail is revealed at all the different levels.


Of course, when confronted with complexity, our first instinct is to *reduce* and to treat these in isolation, and we've tried to come to terms with the complexity within fields by creating **specializations**, which are essentially sub-fields. 

This is why a microbiologist seems to speak a completely different language from an ecologist. This might make you wonder whether the two fields are even related; one could argue that they both view the world through a radically different set of lenses.

As human knowledge progresses and we're able to understand systems at different levels, we're continuously spawning newer and newer specializations and hyperspecializations.

Nowhere is this concept clearer than in computer science where incredibly complex systems have been built to deliver cat videos to your screen. There's nothing special about my choice of computer science here, as I'm sure there are similar levels of complexity lurking beneath the surface of any domain.

You could spend years and years in deep study and you still wouldn't really fully *get* how a computer worked. There would always be gaps in your knowledge. Even behind something as seemingly simple as allowing you to read these words on your screen, there is so much hidden complexity. There are towers and towers of abstractions that enable this to happen:

1. Your CPU (essentially a tiny piece of silicon that can't do much but add two numbers together) executed a few million instructions during the time you were reading this sentence. Welcome to Computer Architecture!
2. These instructions are in most cases not understandable by a human. So we invented a "high-level language" and wrote another software to convert this high level language into the instructions. Welcome to Compilers!
3. There are multiple applications running on this system at the same time, somehow somebody (the Operating System) managed to abstract all this away such that the applications are able to pretend that they're the only application running on the system. Similarly, there are other resources (RAM, files, I/O devices) that all need to be shared between the hundreds of programs running on your system. Welcome to Operating Systems!
4. Your device doesn't exist in isolation. In fact, a lot of its capabilities arise in relation with other devices. Just for the purpose of loading this website and viewing it, it had to send signals out into the ether which somehow (like a labelled envelope) found its way to the right servers and then they responded back with the data you were requesting. Welcome to Networks!

In fact, all of the cases above are huge simplifications. I haven't even begun scratching the surface. If you're curious to learn more, [look at how much detail](https://github.com/alex/what-happens-when) is hidden behind a simple Google search.

A short digression here: there is a famous result in psychology on working memory (also called [Miller's Law](https://en.wikipedia.org/wiki/The_Magical_Number_Seven,_Plus_or_Minus_Two)). It suggests that humans can, on average, hold about 7 objects in their short-term memory at one time. Basically, if you're playing around with concepts in your head and thinking about how things relate with one another, there seems to be a cognitive limit of about 7 items.

Of course that is quite fuzzy and it is of course dangerous to generalize too much from any one result. However, I'd suggest that we can take away this lesson from Miller's Law: humans can't hold too many things in their head at the same time. Software systems are very, very complex beasts and it's beyond the scope of any person to hold in their head all the minute details of how something is working. 

So how do you even begin to understand and make sense of things which seems to have so many interconnected pieces?

This is where *Abstraction* comes in. This is one of the fundamental building blocks of Computer Science, Engineering, and Problem Solving in general. Abstraction is when you squint your eyes and treat something as a *black box*. You are temporarily choosing to not care about what's happening inside the black box because other details are more important. 

For an example of abstraction think about *interfaces* to objects you commonly use. A car for example hides a lot of complicated circuitry and machinery, but at the end of the day, all you need to care about is the steering wheel and a couple of pedals. That's the abstract view of a car. I don't care how that car turns these inputs into the multiple complicated outputs of fuel intake, torque etc. I completely ignore that because it's not important to me. I just want to get from point A to point B and I just need to know how to use the interface of the car to achieve the goal.

A well-designed interface (this is true of software interfaces too!) would allow you to focus on fewer aspects of the car and expend lesser *cognitive effort* when driving the car; I'm talking about manual vs automatic vs self-driving cars. 

Sure, that's all well and good, but this article is about *Dealing with Complexity*, not about *Pretending it doesn't exist*. After all, someone does ultimately have to design and work with the underlying complexity ([Tesler's Law](https://en.wikipedia.org/wiki/Law_of_conservation_of_complexity)). You'd be a terrible automobile engineer if you were only able to see a car as its interface!

Things are even harder when you're a student and trying to learn more about a certain field you have no prior knowledge in; with so much to learn, how do you decide what is worth digging deeper into?

So let's talk about Fractal Learning, which is the focus of this blog post.

## Part 2 - Fractal Learning

I first came across this concept in the excellent [IntermezzOS Book](https://intermezzos.github.io/book/second-edition/fractal-learning.html). It very beautifully encapsulates the challenges of students and researchers working to make sense of complex systems:

> It's impossible to learn everything at once. If you keep digging, you'll find more questions, and digging into those questions leads to more questions... at some point, you have to say "okay, I know enough about this for now, it's time to move on."

Here's that picture of a tree again:

![tree]({static}/images/tree.gif)

Let's imagine that this tree represents your field of interest. Your job is to somehow navigate this tree and learn *everything*. Now, clearly learning everything isn't going to happen in this lifetime, but you still want to know enough to get a good idea of how everything fits together.

How do you do it?

Most people seem to follow one of two strategies - and these strategies come under the umbrella of *tree-traversal algorithms* in computer science. 

The first is depth-first search (I call it *falling down the rabbit hole*). It's represented by this animation[ref]I got these visualizations from [this link](https://google.com)[/ref].

<center>![dfs]({static}/images/dfs.gif)</center>

Here, you start at the root of the tree (represented by 1) and you keep going deeper and deeper along any one path. You stop when you can go no further and then try one of the other paths.

To better visualize it, here is the same algorithm working to solve a maze:[ref]from [this link](https://coderscat.com/depth-first-searchdfs-explained-with-visualization)[/ref]

<center>![dfs]({static}/images/dfsmaze.gif)</center>

I hope you get why I called it falling down the rabbit hole; you're just digging deeper and deeper onto a specific topic and not really taking a step back and exploring other related topics before going deeper. This is a laser-focused, strategy that focuses on exclusion. It's the equivalent of the child who keeps asking "Why?" for a specific topic until he can no longer get a meaningful answer. If someone followed only this strategy for learning about things, he would have very detailed and specific knowledge about some things but may have absolutely no idea about even closely related things.

Let's talk about the other strategy - breadth-first search (I call it *flooding*):

<center>![dfs]({static}/images/bfs.gif)</center>

This is the polar opposite of the previous approach. Here you are quite timid, and you never go more than one level at a time, and you make your way level-by-level.

Here it is again, in the maze context:

<center>![dfs]({static}/images/bfsmaze.gif)</center>

Again, I hope it makes sense why I called it flooding; it works similarly to how water slowly rises and floods an area. Here you dig very shallowly and take the time to understand the very basics of everything before making your way deeper. This is focused on being inclusive and getting a sense of how everything fits together, even if it's not relevant. Someone who followed only this strategy would be a jack of all trades, she would know the very fundamentals of everything but like the figure of speech goes, she would be a master of none.


So which should you follow: breadth-first or depth-first? Sadly, as with a lot of things in life, the answer is: it depends. You will have to use your judgment and intuition to decide which approach to use depending on the situation. Both approaches have their trade-offs. 

Of course, given enough time, both approaches would cover the entire tree. As we saw, both approaches managed to solve the maze.

But we don't really have much time, do we?

When you are new to a field and trying to get an overall sense of how the field works, it might be a waste of time to fall down any rabbit holes as you don't have the experience to know which lines of questioning are relevant. So a breadth-centric approach might serve you well. Notice how I said breadth-centric, and not breadth-first. Going breadth-first is mostly going to be more trouble than it's worth as there are too many topics to even skim through. In this case you would form appropriate abstractions for the concepts you are encountering so that you can dig deeper into them later if you need.

On the other hand, let's say you are trying to find a specific solution to a problem that you are trying to solve. Here it might be wiser to follow a more depth-centric approach, using your intuition to guide you down relevant lines of questioning. Note that it's much better to have an overall sense of how things work; otherwise, you wouldn't know where to start, and it would just be an undirected (and probably fruitless) search. Here, it would just be a distraction to think too much about the things you are encountering. It may save you some time to skim the material and get an overall sense of how things work, in case you need it later. However, it is very important to stay on track and not lose track of the specific thing you are trying to solve at that moment.

Ultimately these examples really demonstrate what the topic of this blog post is about: Fractal learning.

Fractal learning is ultimately about balance and flexibility. It's about being able to:

- keep the big picture in your mind even when digging into the details,

- paying attention to the details even when you are just skimming 

- choose the appropriate level of abstraction for what you're trying to do

- zoom in and zoom out of different levels of abstraction. 

It means that sometimes you choose to accept things the way things are without questioning deeper, and maybe later you choose to dig deeper into those same assumptions.

This might seem by its very nature to be quite contradictory and there's definitely a very Zen aspect to it. This is not an easy or simple thing to do, and it is natural and expected that you sometimes you go too deeply into irrelevant details and too shallowly into the important details. But this is definitely a skill that can be learned. It's not just a skill. It's a *meta-skill* which would greatly increase the rate at which you learn all new skills.

With the increasing amount of information we have at our disposal, this is more relevant than ever. It is all too easy to get lost and fail to see the forest for the trees.

This is exactly what the book Range also talks about:

> The challenge we all face is how to maintain the benefits of breadth, diverse experience, interdisciplinary thinking, and delayed concentration in a world that increasingly incentivizes, even demands, hyperspecialization.
>
> From the book Range - David Epstein

In general I have seen a tendency in myself to dig too deep into a topic early on and get lost in the maze. I hope this article encourages you to pause, take a step back, and think about the overall system every once in a while.

I'll conclude with another snippet from the same book that inspired this article in the first place.

> It's okay to not "fully" understand something before moving on, whatever that means. You'll get back to it. Sometimes, learning something else is more important than diving into every last detail.
